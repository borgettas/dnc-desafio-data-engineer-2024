# Objetivo do Projeto
    Contruir um pipeline de dodos que envolva as seguintes etapas:
1. **Extra√ß√£o de Dados:** Extrair dados sobre o [clima](https://open-meteo.com/en/docs#latitude=-23.1794&longitude=-45.8869&timezone=America%2FSao_Paulo) e [tr√¢nsito](https://developers.google.com/maps/documentation/directions/overview);
2. **Limpeza e Transforma√ß√£o**;
3. **Modelagem de Dados**;
4. **Integra√ß√£o com o Banco de Dados**;
5. **Visualiza√ß√£o de Dados**;


# Arquitetura do projeto
![alt text](docs/images/arquitetura_projeto.png)


## API's
1. **Open-Meteo**: API de simples consulta em python, onde a mesma n√£o precisa de autentica√ß√£o e √© gratuita, nos disponibilizando diversas categorias de dados hist√≥ricos como:
    
    - Temperatura
    - Umidade Relativa
    - Chuva
    - Vento 

2. **Google Diretions**: API do Google respons√°vel por nos entregar os trajetos de um ponto **A** ao ponto **B**. Nela √© necess√°rio o acesso ao console do Google Cloud e cria√ß√£o de uma *key* para autentica√ß√£o na API.

## Orquestrador
Para orquestrar nossas tarefas de maneira autom√°tica, foi utilizado o orquestrador Open-Source `Apache Airflow`. Nele criaremos algumas tarefas que far√£o a requisi√ß√£o nas api's e conex√£o com o banco de dados para salvar os dados coletados no banco de dados.

## Banco de Dados
Par o nosso banco de dados relacional(Data Warehouse), vamos utilizar um tradicional e popular `Postgres 16`, onde nele, ser√£o criadas tabelas para cada segmento de cada dado advindo das api's.

## Data Viz
E para vizualiza√ß√£o dos nossos dados vamos utilizar o `Metabase`, outra ferramenta Open Source, com facil processo para cria√ß√£o dos nossos dados.

#  Gerenciamento da aplica√ß√µes
Para gerenciar nossos apps, vamos constru√≠-los utilizando o Docker-Compose. Com o objetivo de facilitar a instala√ß√£o, manuten√ß√£o e replicabilidade. Para encontrar as configura√ß√µes, acesse `docker/` .

`üí° FYI` Quando o Docker-Compose iniciar o Postgres, as tabelas ser√£o criadas automaticamente e as DAG's tamb√©m ser√£o carregadas para o airflow de forma autom√°tica. 


# Modelagem dos Dados 
Como em nosso exemplo estamos usandos poucos dados os modelos conceituais e l√µgicos acabaram sendo menos relevantes.

## F√≠sico
![alt text](docs/data/Untitled.svg)


# Requisitos
Esse ambiente foi construido sobre o linux e precisaremos das seguintes tecnologias:

- Linux;
- Docker e Docker-Compose;
- Pip;
- Make
- Python;
- Navegador;
- Conex√£o Internet;
- Dbeaver, Postgres ou similares;


# Instalando o ambiente
Primeiramente, precisaremos criar algumas variaveis de ambiente na sua m√°quina, por isso execute o comando a seguir no seu terminal:

    export POSTGRES_DW_USER=postgres
    export POSTGRES_DW_PASSWORD=postgres
    export POSTGRES_DW_HOST=datawarehouse
    export POSTGRES_DW_PORT=5432
    export POSTGRES_DW_GOOGLE_API_KEY="CHAVE_GERADA_NO_GOOGLE_CLOUD"


Com isso, podemos iniciar nosso projeto com:

    make run


Para iniciar nossos pipelines acesse utilizando o **Username** `airflow` e **Password** `airflow` 
    
    http://localhost:8090/

![alt text](docs/images/tela_login_airflow.png)


## Executando Pipelines
Assim que estiver no ambiente, inicies as DAGs dispon√≠veis e em poucos segundos elas ja dever√£o estar finalizadas com sucesso.

![alt text](docs/images/dasg_exemplo.png)


## Dados
Os dados que ser√£o coletados pelos pipelines poder√£o ser vizuados tamb√©m pelo Dbeaver ou Postgres:

    USER=postgres
    PASSWORD=postgres
    HOST=localhost
    PORT=5438

# Testes Unit√°rios

Criar  Ambiente Python
	
    virtualenv .venv

Ativar Ambiente Python

	source .venv/bin/activate

Instalar Depend√™ncias

    pip install -r requirements.txt

Rodar Teste

    pytest tests


Desaivar Ambiente Python
	
    deactivate
