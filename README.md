# Objetivo do Projeto
    Contruir um pipeline de dodos que envolva as seguintes etapas:
1. **ExtraÃ§Ã£o de Dados:** Extrair dados sobre o [clima](https://open-meteo.com/en/docs#latitude=-23.1794&longitude=-45.8869&timezone=America%2FSao_Paulo) e [trÃ¢nsito](https://developers.google.com/maps/documentation/directions/overview);
2. **Limpeza e TransformaÃ§Ã£o**;
3. **Modelagem de Dados**;
4. **IntegraÃ§Ã£o com o Banco de Dados**;
5. **VisualizaÃ§Ã£o de Dados**; 


# Arquitetura do projeto
![alt text](docs/images/arquitetura_projeto.png)

    .
    â”œâ”€â”€ dags
    â”‚Â Â  â”œâ”€â”€ commons
    â”‚Â Â  â”œâ”€â”€ dags_files_1.py
    â”‚Â Â  â””â”€â”€ dags_files_2.py
    â”œâ”€â”€ docker
    â”œâ”€â”€ docs
    â””â”€â”€ tests


## API's
1. **Open-Meteo**: API de simples consulta em python, onde a mesma nÃ£o precisa de autenticaÃ§Ã£o e Ã© gratuita, nos disponibilizando diversas categorias de dados histÃ³ricos como:
    
    - Temperatura
    - Umidade Relativa
    - Chuva
    - Vento 

2. **Google Diretions**: API do Google responsÃ¡vel por nos entregar os trajetos de um ponto **A** ao ponto **B**. Nela Ã© necessÃ¡rio o acesso ao console do Google Cloud e criaÃ§Ã£o de uma *key* para autenticaÃ§Ã£o na API.

## Orquestrador
Para orquestrar nossas tarefas de maneira automÃ¡tica, foi utilizado o orquestrador Open-Source `Apache Airflow`. Nele criaremos algumas tarefas que farÃ£o a requisiÃ§Ã£o nas api's e conexÃ£o com o banco de dados para salvar os dados coletados no banco de dados.

## Banco de Dados
Par o nosso banco de dados relacional(Data Warehouse), vamos utilizar um tradicional e popular `Postgres 16`, onde nele, serÃ£o criadas tabelas para cada segmento de cada dado advindo das api's.

## Data Viz
E para vizualizaÃ§Ã£o dos nossos dados vamos utilizar o `Metabase`, outra ferramenta Open Source, com facil processo para criaÃ§Ã£o dos nossos dados.

#  Gerenciamento da aplicaÃ§Ãµes
Para gerenciar nossos apps, vamos construÃ­-los utilizando o Docker-Compose. Com o objetivo de facilitar a instalaÃ§Ã£o, manutenÃ§Ã£o e replicabilidade. Para encontrar as configuraÃ§Ãµes, acesse `docker/` .

`ðŸ’¡ FYI` Quando o Docker-Compose iniciar o Postgres, as tabelas serÃ£o criadas automaticamente e as DAG's tambÃ©m serÃ£o carregadas para o airflow de forma automÃ¡tica. 


# Modelagem dos Dados 
Como em nosso exemplo estamos usandos poucos dados os modelos conceituais e lÃµgicos acabaram sendo menos relevantes.

## FÃ­sico
![alt text](docs/data/Untitled.svg)


# Requisitos
Esse ambiente foi construido sobre o linux e precisaremos das seguintes tecnologias:

- Linux;
- Docker e Docker-Compose;
- Pip;
- Make
- Python;
- Navegador;
- ConexÃ£o Internet;
- Dbeaver, Postgres ou similares;


# Instalando o ambiente
Primeiramente, precisaremos criar algumas variaveis de ambiente na sua mÃ¡quina, por isso execute o comando a seguir no seu terminal:

    export POSTGRES_DW_USER=postgres
    export POSTGRES_DW_PASSWORD=postgres
    export POSTGRES_DW_HOST=datawarehouse
    export POSTGRES_DW_PORT=5432
    export POSTGRES_DW_GOOGLE_API_KEY="CHAVE_GERADA_NO_GOOGLE_CLOUD"


Com isso, podemos iniciar nosso projeto com:

    make run


Para iniciar nossos pipelines acesse utilizando o **Username** `airflow` e **Password** `airflow` 
    
    http://localhost:8090/

![alt text](docs/images/tela_login_airflow.png)


## Executando Pipelines
Assim que estiver no ambiente, inicies as DAGs disponÃ­veis e em poucos segundos elas ja deverÃ£o estar finalizadas com sucesso.

![alt text](docs/images/dasg_exemplo.png)


## Dados
Os dados que serÃ£o coletados pelos pipelines poderÃ£o ser vizuados tambÃ©m pelo Dbeaver ou Postgres:

    USER=postgres
    PASSWORD=postgres
    HOST=localhost
    PORT=5438

## Metabase
Acesse em http://localhost:3000

`TBD`

# Testes UnitÃ¡rios

Criar  Ambiente Python
	
    virtualenv .venv

Ativar Ambiente Python

	source .venv/bin/activate

Instalar DependÃªncias

    pip install -r requirements.txt

Rodar Teste

    pytest tests


Desaivar Ambiente Python
	
    deactivate
